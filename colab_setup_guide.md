# ğŸš€ Google Colab Pro Plus - Local LLM Kurulum Rehberi

Google Colab'da PDF ChatBot projenizi local LLM ile nasÄ±l Ã§alÄ±ÅŸtÄ±racaÄŸÄ±nÄ±za dair detaylÄ± rehber.

## ğŸ¯ Model Ã–nerileri (Colab Pro Plus iÃ§in)

### 1. **Llama 3.1 8B Instruct** (En Ä°deal)
```python
# HuggingFace Login gerekli (Ã¼cretsiz hesap)
!huggingface-cli login --token YOUR_HF_TOKEN

model_name = "meta-llama/Llama-3.1-8B-Instruct"
```

**AvantajlarÄ±:**
- âœ… TÃ¼rkÃ§e desteÄŸi mÃ¼kemmel
- âœ… Finansal analiz iÃ§in optimize
- âœ… 16GB GPU memory (Colab Pro Plus uygun)
- âœ… Meta'nÄ±n en stabil modeli

### 2. **Mistral 7B Instruct** (HÄ±zlÄ± Alternatif)
```python
model_name = "mistralai/Mistral-7B-Instruct-v0.3"
```

**AvantajlarÄ±:**
- âœ… Daha az memory (~14GB)
- âœ… HÄ±zlÄ± inference
- âœ… Avrupa dilleri gÃ¼Ã§lÃ¼
- âš ï¸ TÃ¼rkÃ§e Llama'dan biraz zayÄ±f

### 3. **Llama 3.2 3B** (Hafif SeÃ§enek)
```python
model_name = "meta-llama/Llama-3.2-3B-Instruct"
```

**AvantajlarÄ±:**
- âœ… Ã‡ok hafif (~6GB)
- âœ… HÄ±zlÄ± baÅŸlatma
- âš ï¸ Daha basit reasoning

## ğŸ”§ Colab Kurulum AdÄ±mlarÄ±

### 1. Gerekli KÃ¼tÃ¼phaneleri YÃ¼kleyin
```bash
!pip install transformers accelerate torch bitsandbytes
!pip install gradio chromadb sentence-transformers
!pip install pdfplumber PyPDF2 pandas numpy
```

### 2. GPU KontrolÃ¼
```python
import torch
print(f"GPU Available: {torch.cuda.is_available()}")
print(f"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB" if torch.cuda.is_available() else "No GPU")
```

### 3. HuggingFace Token (Llama iÃ§in)
```python
# https://huggingface.co/settings/tokens adresinden token alÄ±n
from huggingface_hub import login
login(token="your_token_here")
```

### 4. Projeyi Ä°ndirin ve Ã‡alÄ±ÅŸtÄ±rÄ±n
```python
!git clone YOUR_REPO_URL
%cd pdf-chatbot

# ChatBot'u baÅŸlatÄ±n
!python chat_bot.py
```

## ğŸ›ï¸ Memory OptimizasyonlarÄ±

### Quantization (4-bit/8-bit)
```python
# llm_service_local.py dosyasÄ±nda
self.model = AutoModelForCausalLM.from_pretrained(
    self.model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_8bit=True,  # 8-bit quantization
    # load_in_4bit=True,  # Daha agresif
)
```

### Gradient Checkpointing
```python
self.model.gradient_checkpointing_enable()
```

### Memory Cleanup
```python
import gc
import torch

def cleanup_memory():
    gc.collect()
    torch.cuda.empty_cache()
    
# Model deÄŸiÅŸtirirken kullanÄ±n
cleanup_memory()
```

## ğŸŒ Public URL (Colab)

```python
# chat_bot.py dosyasÄ±nda
demo.launch(
    server_name="0.0.0.0",
    server_port=7860,
    share=True,  # Public URL iÃ§in
    debug=False
)
```

## ğŸ“Š Performance KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | Memory | HÄ±z | TÃ¼rkÃ§e | Reasoning | Colab Uyumlu |
|-------|--------|-----|--------|-----------|---------------|
| Llama 3.1 8B | 16GB | Orta | â­â­â­â­â­ | â­â­â­â­â­ | âœ… |
| Mistral 7B | 14GB | HÄ±zlÄ± | â­â­â­â­ | â­â­â­â­ | âœ… |
| Llama 3.2 3B | 6GB | Ã‡ok HÄ±zlÄ± | â­â­â­â­ | â­â­â­ | âœ… |
| Llama 3.1 70B | 40GB | YavaÅŸ | â­â­â­â­â­ | â­â­â­â­â­ | âŒ |

## ğŸš« YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mleri

### 1. "CUDA out of memory"
```python
# Ã‡Ã¶zÃ¼m: Quantization kullanÄ±n
load_in_8bit=True
# veya daha kÃ¼Ã§Ã¼k model seÃ§in
```

### 2. "Gated repo" HatasÄ±
```python
# Ã‡Ã¶zÃ¼m: HuggingFace login
!huggingface-cli login
# veya aÃ§Ä±k model kullanÄ±n
```

### 3. "Model too slow"
```python
# Ã‡Ã¶zÃ¼m: Smaller model veya quantization
model_name = "meta-llama/Llama-3.2-3B-Instruct"
```

## ğŸ¯ Ã–nerilen Colab Workflow

```python
# 1. KÃ¼tÃ¼phaneler
!pip install -r requirements.txt

# 2. GPU Check  
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")

# 3. Model seÃ§imi (memory'ye gÃ¶re)
if torch.cuda.get_device_properties(0).total_memory > 20e9:
    model_choice = "llama_3_1_8b"  # 16GB+ iÃ§in
else:
    model_choice = "llama_3_2_3b"  # Daha az iÃ§in

# 4. ChatBot baÅŸlat
from chat_bot import create_gradio_interface
demo = create_gradio_interface()
demo.launch(share=True)  # Public URL
```

## ğŸ’¡ Pro Tips

1. **Model Cache**: Ä°lk yÃ¼klemede model indirilir, sonraki Ã§alÄ±ÅŸtÄ±rmalarda cache'den alÄ±nÄ±r
2. **Session Persistence**: Colab session kapanÄ±rsa model yeniden yÃ¼klenir
3. **Background Running**: `nohup python chat_bot.py &` ile background'da Ã§alÄ±ÅŸtÄ±rÄ±n
4. **Memory Monitor**: `!nvidia-smi` ile GPU memory takip edin

## ğŸ”— FaydalÄ± Linkler

- [HuggingFace Token](https://huggingface.co/settings/tokens)
- [Llama Models](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)
- [Mistral Models](https://huggingface.co/collections/mistralai/mistral-7b-65f946c6b3aac8b1b8c11ce0)
- [Google Colab Pro](https://colab.research.google.com/signup)

---

Bu rehber ile Google Colab Pro Plus'ta local LLM ile PDF ChatBot'unuzu sorunsuz Ã§alÄ±ÅŸtÄ±rabilirsiniz! ğŸš€ 