#!/usr/bin/env python3
"""
PDF ChatBot - Complete RAG Implementation
Interactive chat interface with PDF upload and question answering
Now supports Local LLM models (Llama & Mistral) via HuggingFace
"""

import os
import logging
import streamlit as st
from typing import List, Optional, Tuple
from dataclasses import dataclass
import tempfile
import json
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="PDF & Excel ChatBot - Local LLM",
    page_icon="ğŸ“ŠğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Import our custom modules
from pdf_processor import PDFProcessor
from text_processor import TextProcessor
from vector_store import VectorStore, RetrievalService
from excel_processor import ExcelProcessor

# Try to import local LLM service
try:
    from llm_service_local import LocalLLMService, create_optimized_llm_service, RECOMMENDED_MODELS
    LOCAL_LLM_AVAILABLE = True
    logger.info("âœ… Local LLM service available")
except ImportError:
    LOCAL_LLM_AVAILABLE = False
    logger.warning("âš ï¸ Local LLM service not available")

# GGUF Model support import'unu diÄŸer import'larÄ±n sonuna ekle
try:
    from llama_cpp import Llama
    GGUF_AVAILABLE = True
    logger.info("âœ… llama-cpp-python available for GGUF models")
except ImportError:
    GGUF_AVAILABLE = False
    logger.warning("âš ï¸ llama-cpp-python not available for GGUF models")

@dataclass
class ChatMessage:
    """Container for chat messages"""
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: datetime
    sources: Optional[List[str]] = None

# Add GGUF service class BEFORE LLMService class
class GGUFModelService:
    """Service for GGUF models using llama-cpp-python - A100 Optimized"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model_name = os.path.basename(model_path)
        self.llm = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize GGUF model with A100 optimizations"""
        try:
            logger.info(f"ğŸš€ Loading GGUF model: {self.model_name} from path: {self.model_path}")
            
            # A100 GPU optimized settings
            import torch
            
            # Detect GPU and set optimal parameters
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                
                if "A100" in gpu_name:
                    # A100 specific optimizations
                    n_ctx = 8192        # Large context window for A100
                    n_batch = 4096      # Large batch size for A100's memory
                    n_threads = 8       # More CPU threads for A100 systems
                    logger.info("ğŸ¯ A100 detected - Using maximum performance settings")
                elif gpu_memory > 20:
                    # High-end GPU settings
                    n_ctx = 6144
                    n_batch = 3072
                    n_threads = 6
                    logger.info("ğŸš€ High-end GPU detected - Using optimized settings")
                else:
                    # Standard GPU settings
                    n_ctx = 4096
                    n_batch = 2048
                    n_threads = 4
                    logger.info("ğŸ“Š Standard GPU detected - Using balanced settings")
            else:
                # CPU fallback
                n_ctx = 2048
                n_batch = 512
                n_threads = 4
                logger.warning("âš ï¸ No GPU detected - Using CPU settings")
            
            # Initialize with optimized settings
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=n_ctx,           # Dynamic context window
                n_gpu_layers=-1,       # Offload all layers to GPU
                n_batch=n_batch,       # Dynamic batch size
                verbose=True,
                n_threads=n_threads,   # Dynamic CPU threads
                # A100 specific optimizations
                use_mmap=True,         # Memory mapping for efficiency
                use_mlock=False,       # Don't lock memory (let OS manage)
                numa=False,            # Disable NUMA for single GPU
                # Performance optimizations
                low_vram=False,        # A100 has plenty of VRAM
                f16_kv=True,          # Use half precision for key-value cache
                logits_all=False,     # Don't compute logits for all tokens
                vocab_only=False,     # Load full model
                # Threading and batching
                n_threads_batch=n_threads,  # Threads for batch processing
            )
            
            logger.info("âœ… GGUF model loaded successfully")
            logger.info(f"ğŸ“Š Context window: {n_ctx}")
            logger.info(f"ğŸ”¢ Batch size: {n_batch}")
            logger.info(f"ğŸ§µ CPU threads: {n_threads}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load GGUF model: {e}")
            self.llm = None
    
    def generate_response(self, query: str, context: str, chat_history=None) -> Tuple[str, float]:
        """Generate response using GGUF model with A100 optimizations"""
        llm_start_time = datetime.now()
        duration_seconds: float = 0.0

        if self.llm is None:
            fallback_response = self._generate_fallback_response(query, context)
            llm_end_time = datetime.now()
            duration_seconds = (llm_end_time - llm_start_time).total_seconds()
            return fallback_response, duration_seconds
        
        try:
            system_prompt = """Sen finansal dokÃ¼manlarÄ± analiz eden uzman bir asistansÄ±n. Verilen baÄŸlam bilgilerini kullanarak TÃ¼rkÃ§e cevap ver.

Kurallar:
- Sadece baÄŸlam bilgilerini kullan
- EÄŸer baÄŸlamda cevap yoksa "Bu bilgi dokÃ¼manlarda bulunmuyor" de
- Finansal terimleri doÄŸru kullan
- SayÄ±sal verileri dikkatli kontrol et
- KÄ±sa ve net cevaplar ver"""

            # Mistral chat format
            prompt = f"<s>[INST] {system_prompt}\n\nBaÄŸlam Bilgileri:\n{context}\n\nSoru: {query} [/INST]"
            
            # A100 optimized generation parameters
            response = self.llm(
                prompt,
                max_tokens=1024,       # Increased for A100
                temperature=0.7,
                top_p=0.95,           # Nucleus sampling
                top_k=40,             # Top-k sampling
                repeat_penalty=1.1,   # Prevent repetition
                stop=["</s>", "[INST]", "[/INST]"],  # Stop sequences
                # A100 performance optimizations
                stream=False,         # Non-streaming for batch efficiency
                echo=False,          # Don't echo prompt
                # Threading for A100
                threads=8 if "A100" in str(self.llm) else 4,
            )
            
            # Extract response text
            if isinstance(response, dict) and 'choices' in response:
                response_text = response['choices'][0]['text'].strip()
            elif isinstance(response, dict) and 'content' in response:
                response_text = response['content'].strip()
            else:
                response_text = str(response).strip()
            
            # Clean up response
            response_text = self._clean_response(response_text)
            
            llm_end_time = datetime.now()
            duration_seconds = (llm_end_time - llm_start_time).total_seconds()
            
            logger.info(f"âœ… GGUF response generated in {duration_seconds:.2f}s")
            return response_text, duration_seconds
            
        except Exception as e:
            logger.error(f"âŒ GGUF generation failed: {e}")
            fallback_response = self._generate_fallback_response(query, context)
            llm_end_time = datetime.now()
            duration_seconds = (llm_end_time - llm_start_time).total_seconds()
            return fallback_response, duration_seconds
    
    def _clean_response(self, response_text: str) -> str:
        """Clean and format the model response"""
        # Remove unwanted tokens and formatting
        response_text = response_text.replace("</s>", "")
        response_text = response_text.replace("[INST]", "")
        response_text = response_text.replace("[/INST]", "")
        response_text = response_text.replace("<s>", "")
        
        # Remove extra whitespace
        response_text = " ".join(response_text.split())
        
        # Remove any repeated patterns
        lines = response_text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in cleaned_lines[-3:]:  # Avoid recent repetitions
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """Fallback when GGUF model fails"""
        if context:
            return f"""
**[GGUF Model HatasÄ±]**

Sorunuz: {query}

Bulunan Ä°lgili Bilgiler:
{context[:500]}...

ğŸ’¡ GGUF model dÃ¼zgÃ¼n yÃ¼klendiÄŸinde bu bilgileri analiz ederek detaylÄ± cevap verebilirim.
"""
        else:
            return "Bu konuda dokÃ¼manlarda ilgili bilgi bulunamadÄ±."
    
    def get_model_info(self) -> dict:
        """Get GGUF model information"""
        return {
            "model_name": self.model_name,
            "model_loaded": self.llm is not None,
            "device": "gpu" if self.llm else "unknown",
            "model_path": self.model_path,
            "service_type": "gguf"
        }

# Modify existing LLMService class - add gguf_model_path parameter
class LLMService:
    """
    LLM service supporting GGUF, OpenAI API and Local models
    """
    
    def __init__(self, 
                 use_local: bool = True,
                 model_choice: str = "llama_3_1_8b",
                 api_key: Optional[str] = None,
                 gguf_model_path: Optional[str] = None):
        
        self.use_local = use_local and LOCAL_LLM_AVAILABLE
        self.model_choice = model_choice
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.gguf_model_path = gguf_model_path
        
        self.local_service = None
        self.gguf_service = None
        self.openai_client = None
        
        self._initialize_service()
    
    def _initialize_service(self):
        """Initialize the appropriate LLM service"""
        
        # Try GGUF model first if path provided
        if self.gguf_model_path and GGUF_AVAILABLE:
            try:
                logger.info(f"ğŸš€ Initializing GGUF model: {self.gguf_model_path}")
                self.gguf_service = GGUFModelService(self.gguf_model_path)
                if self.gguf_service.llm:
                    logger.info("âœ… GGUF model service initialized")
                    return
            except Exception as e:
                logger.error(f"âŒ GGUF model initialization failed: {e}")
        
        # Original logic remains the same
        if self.use_local:
            try:
                logger.info(f"ğŸš€ Initializing local LLM: {self.model_choice}")
                self.local_service = create_optimized_llm_service(self.model_choice)
                logger.info("âœ… Local LLM service initialized")
                return
            except Exception as e:
                logger.error(f"âŒ Local LLM initialization failed: {e}")
                logger.info("ğŸ”„ Falling back to OpenAI API...")
        
        # Fallback to OpenAI
        if self.api_key:
            try:
                import openai
                self.openai_client = openai.OpenAI(api_key=self.api_key)
                logger.info("âœ… OpenAI client initialized")
            except ImportError:
                logger.warning("OpenAI library not installed")
            except Exception as e:
                logger.warning(f"OpenAI client initialization failed: {e}")
        else:
            logger.warning("No OpenAI API key provided")
    
    def generate_response(self, query: str, context: str, chat_history: List[ChatMessage] = None) -> Tuple[str, Optional[float]]:
        """
        Generate response using available LLM service and return response with its duration.
        """
        response_text: str
        duration_seconds: Optional[float] = None
        
        # Try GGUF model first
        if self.gguf_service and self.gguf_service.llm:
            try:
                response_text, duration_seconds = self.gguf_service.generate_response(query, context, chat_history)
                return response_text, duration_seconds
            except Exception as e:
                logger.error(f"GGUF model failed: {e}")
        
        # Try local LLM
        if self.local_service:
            try:
                # Placeholder: Modify LocalLLMService.generate_response to return Tuple[str, float] as well
                # response_text, duration_seconds = self.local_service.generate_response(query, context, chat_history)
                # return response_text, duration_seconds
                logger.warning("LocalLLMService duration timing not implemented yet in this refactor.")
                response_text = self.local_service.generate_response(query, context, chat_history)
                return response_text, None # Duration not available yet for this path
            except Exception as e:
                logger.error(f"Local LLM failed: {e}")
        
        # Try OpenAI API
        if self.openai_client:
            try:
                openai_start_time = datetime.now()
                response_text = self._generate_openai_response(query, context, chat_history)
                openai_end_time = datetime.now()
                duration_seconds = (openai_end_time - openai_start_time).total_seconds()
                return response_text, duration_seconds
            except Exception as e:
                logger.error(f"OpenAI API failed: {e}")
        
        # Fallback response
        fallback_start_time = datetime.now()
        response_text = self._generate_fallback_response(query, context)
        fallback_end_time = datetime.now()
        duration_seconds = (fallback_end_time - fallback_start_time).total_seconds()
        return response_text, duration_seconds
    
    def get_service_info(self) -> dict:
        """Get information about the current LLM service"""
        
        if self.gguf_service and self.gguf_service.llm:
            return self.gguf_service.get_model_info()
        elif self.local_service:
            local_info = self.local_service.get_model_info()
            return {
                "service_type": "local",
                "model_name": local_info["model_name"],
                "status": "active" if local_info["model_loaded"] else "failed",
                "device": local_info.get("device", "unknown"),
                "gpu_info": local_info.get("gpu_name", "N/A")
            }
        elif self.openai_client:
            return {
                "service_type": "openai_api",
                "model_name": "gpt-3.5-turbo",
                "status": "active"
            }
        
        return {
            "service_type": "unknown",
            "model_name": "none",
            "status": "not_available"
        }

    # Keep existing methods unchanged...
    def _generate_openai_response(self, query: str, context: str, chat_history: List[ChatMessage] = None) -> str:
        """Generate response using OpenAI API"""
        
        system_prompt = """Sen finansal dokÃ¼manlarÄ± analiz eden uzman bir asistansÄ±n. 
        Sana verilen baÄŸlam bilgilerini kullanarak kullanÄ±cÄ±nÄ±n sorularÄ±nÄ± TÃ¼rkÃ§e olarak cevapla.
        
        Kurallar:
        - Sadece verilen baÄŸlam bilgilerini kullan
        - EÄŸer baÄŸlamda cevap yoksa "Bu bilgi dokÃ¼manlarda bulunmuyor" de
        - Finansal terimleri doÄŸru kullan
        - KaynaklarÄ± belirt
        - KÄ±sa ve net cevaplar ver
        """
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if chat_history:
            for msg in chat_history[-5:]:
                messages.append({"role": msg.role, "content": msg.content})
        
        user_message = f"""
        BaÄŸlam Bilgileri:
        {context}
        
        KullanÄ±cÄ± Sorusu:
        {query}
        """
        
        messages.append({"role": "user", "content": user_message})
        
        response = self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """Fallback response when no LLM available"""
        
        if context:
            return f"""
            **[Demo Modu - LLM servisi mevcut deÄŸil]**
            
            Sorunuz: {query}
            
            Bulunan Ä°lgili Bilgiler:
            {context[:500]}...
            
            ğŸ’¡ LLM servisi aktif olduÄŸunda bu bilgileri analiz ederek detaylÄ± cevap veririm.
            """
        else:
            return "Bu konuda dokÃ¼manlarda ilgili bilgi bulunamadÄ±."

# Update PDFChatBot constructor
class PDFChatBot:
    """
    Main ChatBot class with GGUF support
    """
    
    def __init__(self, use_local_llm: bool = True, model_choice: str = "llama_3_1_8b", gguf_model_path: str = None):
        # Initialize components
        self.pdf_processor = PDFProcessor()
        self.excel_processor = ExcelProcessor()
        self.text_processor = TextProcessor(chunk_size=800, overlap_size=150)
        self.vector_store = VectorStore()
        
        # Create a simple embedding service wrapper
        class EmbeddingServiceWrapper:
            def __init__(self, model):
                self.model_name = model.model_name if hasattr(model, 'model_name') else str(model)
                self.model = model
            
            def generate_embedding(self, text: str):
                return self.model.encode(text)
        
        # Initialize retrieval service with wrapper
        embedding_wrapper = EmbeddingServiceWrapper(self.text_processor.model)
        self.retrieval_service = RetrievalService(
            vector_store=self.vector_store,
            embedding_service=embedding_wrapper
        )
        
        # Initialize LLM service with GGUF support
        self.llm_service = LLMService(
            use_local=use_local_llm,
            model_choice=model_choice,
            gguf_model_path=gguf_model_path
        )
        
        # Chat state
        self.chat_history: List[ChatMessage] = []
        self.processed_files: List[str] = []
        
        logger.info("âœ… PDF ChatBot initialized with GGUF support")
    
    def process_pdf_file(self, pdf_file) -> Tuple[str, Optional[float]]:
        """Process uploaded PDF file and add to vector store. Returns summary and duration."""
        process_start_time = datetime.now()
        duration_seconds: Optional[float] = None

        if pdf_file is None:
            return "âŒ LÃ¼tfen bir PDF dosyasÄ± yÃ¼kleyin.", None
        
        if pdf_file.name in self.processed_files:
            return f"â„¹ï¸ {pdf_file.name} zaten iÅŸlenmiÅŸ. Yeni sorular sorabilirsiniz.", None
        
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(pdf_file.read())
                temp_path = tmp_file.name
            
            logger.info(f"Processing uploaded PDF: {pdf_file.name}")
            pdf_result = self.pdf_processor.process_pdf(temp_path)
            
            embedded_chunks = self.text_processor.process_document_pages(
                pdf_result.pages, 
                pdf_file.name
            )
            
            self.vector_store.add_documents(embedded_chunks)
            self.processed_files.append(pdf_file.name)
            os.unlink(temp_path)
            
            stats = self.text_processor.get_processing_stats(embedded_chunks)
            vector_stats = self.retrieval_service.get_retrieval_stats()
            logger.info(f"Vector store now contains {vector_stats['vector_store_stats'].get('total_documents', 0)} documents")
            
            summary = f"""
            âœ… **PDF baÅŸarÄ±yla iÅŸlendi!**
            
            ğŸ“Š **Ä°statistikler:**
            - ğŸ“„ Sayfa sayÄ±sÄ±: {pdf_result.total_pages}
            - ğŸ§© Toplam chunk: {stats['total_chunks']}
            - ğŸ“ Metin chunks: {stats.get('text_chunks', 0)}
            - ğŸ“Š Tablo chunks: {stats.get('table_chunks', 0)}
            - ğŸ“š Tablolu sayfalar: {stats.get('pages_with_tables', 0)}
            - ğŸ“ Toplam karakter: {stats['total_characters']:,}
            - ğŸŒ Dil: {', '.join(stats['language_distribution'].keys())}
            - ğŸ—ƒï¸ Vector store'da: {vector_stats['vector_store_stats'].get('total_documents', 0)} chunk
            
            ğŸ’¬ ArtÄ±k bu dokÃ¼mana ve tablolarÄ±na dair sorular sorabilirsiniz!
            """
            
            process_end_time = datetime.now()
            duration_seconds = (process_end_time - process_start_time).total_seconds()
            return summary, duration_seconds
            
        except Exception as e:
            logger.error(f"PDF processing failed: {e}")
            process_end_time = datetime.now() # Record time even on failure for debugging
            duration_seconds = (process_end_time - process_start_time).total_seconds()
            return f"âŒ PDF iÅŸlenemedi: {str(e)}", duration_seconds
    
    def process_excel_file(self, excel_file) -> Tuple[str, Optional[float]]:
        """Process uploaded Excel file and add to vector store. Returns summary and duration."""
        process_start_time = datetime.now()
        duration_seconds: Optional[float] = None

        if excel_file is None:
            return "âŒ LÃ¼tfen bir Excel dosyasÄ± yÃ¼kleyin.", None
        
        if excel_file.name in self.processed_files:
            return f"â„¹ï¸ {excel_file.name} zaten iÅŸlenmiÅŸ. Yeni sorular sorabilirsiniz.", None
        
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(excel_file.name)[1]) as tmp_file:
                tmp_file.write(excel_file.read())
                temp_path = tmp_file.name
            
            logger.info(f"Processing uploaded Excel: {excel_file.name}")
            excel_result = self.excel_processor.process_excel(temp_path)
            
            embedded_chunks = self.text_processor.process_excel_sheets(
                excel_result.sheets, 
                excel_file.name
            )
            
            self.vector_store.add_documents(embedded_chunks)
            self.processed_files.append(excel_file.name)
            os.unlink(temp_path)
            
            summary_data = self.excel_processor.get_excel_summary(excel_result)
            vector_stats = self.retrieval_service.get_retrieval_stats()
            logger.info(f"Vector store now contains {vector_stats['vector_store_stats'].get('total_documents', 0)} documents")
            total_text_length = sum(len(sheet.text_content) for sheet in excel_result.sheets)
            sheet_names = [sheet.sheet_name for sheet in excel_result.sheets]
            
            summary = f"""
            âœ… **Excel dosyasÄ± baÅŸarÄ±yla iÅŸlendi!**
            
            ğŸ“Š **Ä°statistikler:**
            - ğŸ“„ Sayfa sayÄ±sÄ±: {summary_data['total_sheets']}
            - ğŸ“ Toplam satÄ±r: {summary_data['total_rows']}
            - ğŸ“‹ Toplam sÃ¼tun: {summary_data['total_columns']}
            - ğŸ”¢ SayÄ±sal sÃ¼tunlar: {summary_data['total_numeric_columns']}
            - ğŸ§© Toplam chunk: {len(embedded_chunks)}
            - ğŸ“ Toplam karakter: {total_text_length:,}
            - ğŸ—ƒï¸ Vector store'da: {vector_stats['vector_store_stats'].get('total_documents', 0)} chunk
            - ğŸ“Š Sayfalar: {', '.join(sheet_names)}
            
            ğŸ’¬ ArtÄ±k bu Excel dosyasÄ±ndaki verilere dair sorular sorabilirsiniz!
            
            **ğŸ“‹ Ã–rnek Sorular:**
            - "Hangi sayfalarda hangi veriler var?"
            - "Toplam satÄ±r sayÄ±sÄ± kaÃ§?"
            - "En yÃ¼ksek deÄŸer nedir?"
            - "Tablo verilerini Ã¶zetle"
            """
            
            process_end_time = datetime.now()
            duration_seconds = (process_end_time - process_start_time).total_seconds()
            return summary, duration_seconds
            
        except Exception as e:
            logger.error(f"Excel processing failed: {e}")
            process_end_time = datetime.now() # Record time even on failure for debugging
            duration_seconds = (process_end_time - process_start_time).total_seconds()
            return f"âŒ Excel dosyasÄ± iÅŸlenemedi: {str(e)}", duration_seconds
    
    def chat(self, query: str, chat_history_display: List[List[str]]) -> Tuple[str, List[List[str]]]:
        """Handle chat interaction"""
        if not query.strip():
            return "", chat_history_display
        
        try:
            # Retrieve relevant context
            context_result = self.retrieval_service.retrieve_context(
                query=query,
                n_results=5
            )
            
            # Generate response
            response, duration_seconds = self.llm_service.generate_response(
                query=query,
                context=context_result.combined_context,
                chat_history=self.chat_history
            )
            
            # Add sources information if available
            if context_result.results:
                sources = []
                for result in context_result.results[:3]:  # Top 3 sources
                    source_info = f"ğŸ“„ {result.source_file} (Sayfa {result.page_number})"
                    sources.append(source_info)
                
                response += f"\n\n**Kaynaklar:**\n" + "\n".join(sources)
            
            # Update chat history
            user_msg = ChatMessage(
                role="user",
                content=query,
                timestamp=datetime.now()
            )
            
            assistant_msg = ChatMessage(
                role="assistant", 
                content=response,
                timestamp=datetime.now(),
                sources=[r.source_file for r in context_result.results]
            )
            
            self.chat_history.extend([user_msg, assistant_msg])
            
            # Update display history
            if chat_history_display is None:
                chat_history_display = []
            
            chat_history_display.append([query, response])
            
            return "", chat_history_display
            
        except Exception as e:
            logger.error(f"Chat processing failed: {e}")
            error_response = f"âŒ Cevap oluÅŸturulamadÄ±: {str(e)}"
            
            if chat_history_display is None:
                chat_history_display = []
                
            chat_history_display.append([query, error_response])
            return "", chat_history_display
    
    def generate_single_response(self, query: str) -> Tuple[str, Optional[float], Optional[float]]:
        """
        Generate a single response for the given query, returning response, total duration, and LLM duration.
        Optimized version with smart retrieval for large documents
        """
        total_query_start_time = datetime.now()
        llm_duration_seconds: Optional[float] = None

        if not query.strip():
            return "âŒ LÃ¼tfen bir soru girin.", (datetime.now() - total_query_start_time).total_seconds(), None
        
        try:
            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Top-k (n_results) 3 olarak sabitlendi >>>
            n_results = 3 
            logger.info(f"ğŸ” Retrieving top {n_results} results for query.")
            # Eski dinamik n_results mantÄ±ÄŸÄ± yorum satÄ±rÄ± haline getirildi veya kaldÄ±rÄ±ldÄ±.
            # vector_stats = self.retrieval_service.get_retrieval_stats()
            # total_docs = vector_stats['vector_store_stats'].get('total_documents', 0)
            # 
            # n_results = 5 # VarsayÄ±lan
            # if total_docs > 200: n_results = 8
            # elif total_docs > 100: n_results = 6
            # 
            # if n_results != 5: logger.info(f"ğŸ” Document mode: {total_docs} chunks, retrieving {n_results} results")
            # <<< GeliÅŸtirme Sonu: Top-k (n_results) 3 olarak sabitlendi >>>
            
            context_result = self.retrieval_service.retrieve_context(query=query, n_results=n_results)
            
            if not context_result.combined_context.strip():
                return "âŒ Bu soruya cevap verebilmek iÃ§in ilgili bilgi dokÃ¼manlarda bulunamadÄ±. LÃ¼tfen farklÄ± bir soru deneyin.", (datetime.now() - total_query_start_time).total_seconds(), None
            
            # Context filtering logic (can be kept or adjusted)
            vector_stats = self.retrieval_service.get_retrieval_stats() # For total_docs in filtering
            total_docs_for_filtering = vector_stats['vector_store_stats'].get('total_documents', 0)
            if total_docs_for_filtering > 200 and len(context_result.combined_context) > 3000: # Bu eÅŸik deÄŸerleri ayarlanabilir
                context_parts = context_result.combined_context.split('\n\n')
                filtered_context = ""
                for part in context_parts:
                    if len(filtered_context + part) < 2000: # Bu karakter limiti de ayarlanabilir
                        filtered_context += part + "\n\n"
                    else: break
                if filtered_context:
                    context_result.combined_context = filtered_context.strip()
                    logger.info(f"ğŸ“ Context filtered for efficiency: {len(context_result.combined_context)} chars")
            
            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: LLM'e giden prompt'u loglama >>>
            logger.info(f"--- LLM INPUT START ---")
            logger.info(f"QUERY: {query}")
            logger.info(f"CONTEXT (first 500 chars): {context_result.combined_context[:500]}...")
            logger.info(f"CONTEXT (length): {len(context_result.combined_context)} chars")
            # GGUFModelService iÃ§indeki prompt formatÄ±nÄ± burada da loglayabiliriz (opsiyonel)
            # system_prompt_for_log = "Sen finansal dokÃ¼manlarÄ± analiz eden uzman bir asistansÄ±n..."
            # full_prompt_for_log = f"<s>[INST] {system_prompt_for_log}\\n\\nBaÄŸlam Bilgileri:\\n{context_result.combined_context}\\n\\nSoru: {query} [/INST]"
            # logger.info(f"FULL PROMPT (length): {len(full_prompt_for_log)} chars")
            logger.info(f"--- LLM INPUT END ---")
            # <<< GeliÅŸtirme Sonu: LLM'e giden prompt'u loglama >>>

            response, llm_duration_seconds = self.llm_service.generate_response(
                query=query,
                context=context_result.combined_context,
                chat_history=self.chat_history
            )
            
            if context_result.results:
                sources = [f"ğŸ“„ {result.source_file} (Sayfa {result.page_number})" for result in context_result.results[:3]]
                response += f"\n\n**Kaynaklar:**\n" + "\n".join(sources)
            
            user_msg = ChatMessage(role="user", content=query, timestamp=datetime.now())
            assistant_msg = ChatMessage(
                role="assistant", 
                content=response,
                timestamp=datetime.now(),
                sources=[r.source_file for r in context_result.results] if context_result.results else []
            )
            self.chat_history.extend([user_msg, assistant_msg])
            
            total_query_duration_seconds = (datetime.now() - total_query_start_time).total_seconds()
            return response, total_query_duration_seconds, llm_duration_seconds
            
        except Exception as e:
            logger.error(f"Single response generation failed: {e}")
            total_query_duration_seconds = (datetime.now() - total_query_start_time).total_seconds()
            return f"âŒ Cevap oluÅŸturulamadÄ±: {str(e)}\n\nLÃ¼tfen tekrar deneyin veya farklÄ± bir soru sorun.", total_query_duration_seconds, None
    
    def get_stats(self) -> str:
        """Get current chatbot statistics including table information and timing."""
        try:
            retrieval_stats = self.retrieval_service.get_retrieval_stats()
            vector_store_actual_stats = retrieval_stats['vector_store_stats'] # Stats from FAISS via VectorStore.get_collection_stats()
            llm_info = self.llm_service.get_service_info()
            
            # Stats from TextProcessor for the last processed file (if any)
            current_processing_stats = self.text_processor.get_processing_stats()

            # Timing Information (from session_state)
            model_load_time = st.session_state.get('model_load_duration_seconds')
            file_process_time = st.session_state.get('last_file_processing_duration_seconds')
            query_response_time = st.session_state.get('last_query_response_duration_seconds')
            llm_inference_time = st.session_state.get('last_llm_inference_duration_seconds')
            
            timing_info = "\n\n**â±ï¸ Son Ä°ÅŸlem SÃ¼releri:**\n"
            if model_load_time is not None: timing_info += f"- Model YÃ¼kleme: {model_load_time:.2f} s\n"
            if file_process_time is not None: timing_info += f"- Son Dosya Ä°ÅŸleme: {file_process_time:.2f} s\n"
            if query_response_time is not None: timing_info += f"- Son Sorgu YanÄ±tlama (Toplam): {query_response_time:.2f} s\n"
            if llm_inference_time is not None: timing_info += f"- Son LLM Ã‡Ä±karÄ±mÄ±: {llm_inference_time:.2f} s\n"
            if timing_info == "\n\n**â±ï¸ Son Ä°ÅŸlem SÃ¼releri:**\n": timing_info = "\n\n**â±ï¸ Son Ä°ÅŸlem SÃ¼releri:**\n- HenÃ¼z bir iÅŸlem yapÄ±lmadÄ±.\n"

            # Use content_type_distribution from vector_store_actual_stats for overall view
            overall_content_distribution = vector_store_actual_stats.get('content_type_distribution', {})
            text_chunks_overall = 0
            table_chunks_overall = 0
            excel_chunks_overall = 0
            for ctype, count in overall_content_distribution.items():
                if ctype == 'text_pdf': text_chunks_overall += count
                elif ctype == 'table_pdf' or ctype == 'table_pdf_fragment': table_chunks_overall += count
                elif ctype == 'excel': excel_chunks_overall += count
                # Add other types if they exist and need to be categorized

            # Constructing the stats text
            stats_text = f"""
            ## ğŸ“Š ChatBot Ä°statistikleri
            
            **ğŸ¤– LLM Servisi:**
            - Tip: {llm_info['service_type']}
            - Model: {llm_info['model_name']}
            - Durum: {llm_info['status']}
            - Cihaz: {llm_info.get('device', 'N/A')}
            - GPU: {llm_info.get('gpu_info', 'N/A')}
            
            **ğŸ“š DokÃ¼man Bilgileri (Vector Store Genel):**
            - Toplam Chunk (Vector Store): {vector_store_actual_stats.get('total_documents', 0)}
            - Metin Chunk (Genel Tahmini): {text_chunks_overall}
            - Tablo Chunk (Genel Tahmini): {table_chunks_overall}
            - Excel Chunk (Genel Tahmini): {excel_chunks_overall}
            - Embedding Modeli: {retrieval_stats['embedding_model']}
            - Ä°ÅŸlenen Dosyalar: {len(self.processed_files)}
            - Dosya Listesi: {', '.join(self.processed_files) if self.processed_files else 'HenÃ¼z dosya yÃ¼klenmedi'}
            
            **ğŸ“„ Son Ä°ÅŸlenen Dosya DetaylarÄ± (TextProcessor):**
            - Toplam Chunk (Son Ä°ÅŸlem): {current_processing_stats.get('total_chunks', 'N/A')}
            - Metin Chunk (Son Ä°ÅŸlem): {current_processing_stats.get('text_chunks', 'N/A')}
            - Tablo Chunk (Son Ä°ÅŸlem): {current_processing_stats.get('table_chunks', 'N/A')}
            - Ä°Ã§erik DaÄŸÄ±lÄ±mÄ± (Son Ä°ÅŸlem): {json.dumps(current_processing_stats.get('content_type_distribution', {}), indent=2, ensure_ascii=False)}

            **ğŸ”¤ Dil DaÄŸÄ±lÄ±mÄ± (Vector Store Genel - Ã–rneklemden):**
            {json.dumps(vector_store_actual_stats.get('language_distribution', {}), indent=2, ensure_ascii=False)}
            (Ã–rneklem Boyutu: {vector_store_actual_stats.get('sample_size_for_distributions', 'N/A')})
            
            **ğŸ“Š Ä°Ã§erik TÃ¼rÃ¼ DaÄŸÄ±lÄ±mÄ± (Vector Store Genel - Ã–rneklemden):**
            {json.dumps(overall_content_distribution, indent=2, ensure_ascii=False)}
            (Ã–rneklem Boyutu: {vector_store_actual_stats.get('sample_size_for_distributions', 'N/A')})
            {timing_info}
            """
            
            return stats_text
            
        except Exception as e:
            logger.error(f"Error getting stats: {e}", exc_info=True)
            return f"âŒ Ä°statistik alÄ±namadÄ±: {str(e)}"

def main():
    """Main Streamlit application with GGUF support"""
    
    # Initialize session state
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'model_initialized' not in st.session_state:
        st.session_state.model_initialized = False
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = []
    if 'current_query' not in st.session_state:
        st.session_state.current_query = ""
    # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Zamanlama iÃ§in Session State DeÄŸiÅŸkenleri >>>
    if 'model_load_duration_seconds' not in st.session_state:
        st.session_state.model_load_duration_seconds = None
    if 'last_file_processing_duration_seconds' not in st.session_state:
        st.session_state.last_file_processing_duration_seconds = None
    if 'last_query_response_duration_seconds' not in st.session_state:
        st.session_state.last_query_response_duration_seconds = None
    if 'last_llm_inference_duration_seconds' not in st.session_state:
        st.session_state.last_llm_inference_duration_seconds = None
    # <<< GeliÅŸtirme Sonu: Zamanlama iÃ§in Session State DeÄŸiÅŸkenleri >>>
    
    # Title
    st.title("ğŸ“„ğŸ’¬ PDF & Excel ChatBot - Local LLM")
    
    st.markdown("""
    **Finansal dokÃ¼manlarÄ±nÄ±zÄ± ve Excel dosyalarÄ±nÄ±zÄ± yÃ¼kleyin, Local LLM ile sorularÄ±nÄ±zÄ± sorun!**
    
    - ğŸ¤– Llama 3.1 8B veya Mistral 7B modelleri
    - ğŸ“„ PDF dosyalarÄ±nÄ±zÄ± yÃ¼kleyin
    - ğŸ“Š Excel dosyalarÄ±nÄ±zÄ± yÃ¼kleyin (.xls, .xlsx, .xlsm)  
    - ğŸ§  AkÄ±llÄ± finansal analiz
    - ğŸ”’ Tamamen local (internet gerekmez)
    """)
    
    # Sidebar for model configuration
    with st.sidebar:
        st.header("âš™ï¸ Model AyarlarÄ±")
        
        # GGUF Model option
        use_gguf = st.checkbox("ğŸ¦™ GGUF Model Kullan (Mistral 7B)", value=True)
        
        if use_gguf:
            st.info("ğŸ“ GGUF Model Path'i giriniz:")
            default_path = "/content/drive/MyDrive/Colab Notebooks/kredi_rag_sistemi/backup/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
            model_path_input = st.text_input(
                "ğŸ”— GGUF Model Path:",
                value=default_path,
                help="Drive'daki .gguf model dosyanÄ±zÄ±n tam path'i"
            )
            
            actual_model_path_for_llama = model_path_input

            if model_path_input:
                if os.path.exists(model_path_input):
                    st.success(f"âœ… GGUF Model bulundu: {model_path_input}")
                    try:
                        file_size = os.path.getsize(model_path_input) / 1e9
                        st.info(f"ğŸ“Š Dosya boyutu: {file_size:.1f}GB")
                    except:
                        pass
                else:
                    st.error(f"âŒ GGUF Model dosyasÄ± bulunamadÄ±: {model_path_input}")
            
            if not GGUF_AVAILABLE:
                st.error("""
                âŒ **llama-cpp-python yÃ¼klÃ¼ deÄŸil!**
                
                Colab'da Ã§alÄ±ÅŸtÄ±rÄ±n:
                ```bash
                !pip install llama-cpp-python
                ```
                """)
            
            if st.button("ğŸš€ GGUF ChatBot'u BaÅŸlat", type="primary", disabled=st.session_state.model_initialized):
                if not GGUF_AVAILABLE:
                    st.error("âŒ llama-cpp-python kÃ¼tÃ¼phanesi gerekli!")
                elif not model_path_input or not os.path.exists(model_path_input):
                    st.error("âŒ GeÃ§erli model path'i girin!")
                else:
                    with st.spinner("GGUF Model yÃ¼kleniyor... (Bu birkaÃ§ dakika sÃ¼rebilir)"):
                        try:
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Modeli Colab yerel diskine kopyalama >>>
                            final_model_path_to_load = model_path_input
                            if model_path_input.startswith("/content/drive/"):
                                local_model_dir = "/content/models_colab_local"
                                if not os.path.exists(local_model_dir):
                                    os.makedirs(local_model_dir)
                                model_filename = os.path.basename(model_path_input)
                                local_model_path = os.path.join(local_model_dir, model_filename)
                                
                                if not os.path.exists(local_model_path):
                                    st.info(f"Model Google Drive'da. {local_model_path} adresine kopyalanÄ±yor...")
                                    import shutil
                                    shutil.copy2(model_path_input, local_model_path)
                                    st.info(f"Model kopyalandÄ±: {local_model_path}")
                                    final_model_path_to_load = local_model_path
                                else:
                                    st.info(f"Model zaten yerel olarak mevcut: {local_model_path}")
                                    final_model_path_to_load = local_model_path
                            
                            logger.info(f"Attempting to load GGUF model into PDFChatBot using path: {final_model_path_to_load}")
                            # <<< GeliÅŸtirme Sonu: Modeli Colab yerel diskine kopyalama >>>

                            load_start_time = datetime.now()
                            st.session_state.chatbot = PDFChatBot(
                                use_local_llm=True,
                                model_choice="gguf_model",
                                gguf_model_path=final_model_path_to_load
                            )
                            load_end_time = datetime.now()
                            st.session_state.model_load_duration_seconds = (load_end_time - load_start_time).total_seconds()
                            
                            llm_info = st.session_state.chatbot.llm_service.get_service_info()
                            st.session_state.model_initialized = True
                            
                            st.success(f"""
                            âœ… **GGUF ChatBot baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!**
                            
                            **ğŸ¤– Model Bilgileri:**
                            - Model: {llm_info.get('model_name', 'Unknown')}
                            - Durum: {llm_info.get('status', 'Unknown')}
                            - Device: {llm_info.get('device', 'Unknown')}
                            
                            ğŸ’¬ ArtÄ±k PDF yÃ¼kleyip soru sorabilirsiniz!
                            """)
                            
                        except Exception as e:
                            st.error(f"âŒ GGUF ChatBot baÅŸlatÄ±lamadÄ±: {str(e)}")
        
        else:
            # Original HuggingFace model configuration
            model_path = st.text_input(
                "ğŸ“ Model Path:",
                value=os.environ.get("CUSTOM_MODEL_PATH", ""),
                placeholder="/content/drive/MyDrive/model-folder/model.gguf",
                help="Drive'daki model dosyanÄ±zÄ±n tam path'ini girin"
            )
            
            # Model path kontrolÃ¼
            if model_path:
                if os.path.exists(model_path):
                    st.success(f"âœ… Model bulundu!")
                    # Model dosya bilgileri
                    try:
                        file_size = os.path.getsize(model_path) / 1e9
                        st.info(f"ğŸ“Š Dosya boyutu: {file_size:.1f}GB")
                    except:
                        pass
                else:
                    st.error("âŒ Model dosyasÄ± bulunamadÄ±")
            
            # Initialize button
            if st.button("ğŸš€ ChatBot'u BaÅŸlat", type="primary", disabled=st.session_state.model_initialized):
                if not model_path:
                    st.error("âŒ Model path'i girin!")
                elif not os.path.exists(model_path):
                    st.error("âŒ Model dosyasÄ± bulunamadÄ±!")
                else:
                    with st.spinner("Model yÃ¼kleniyor..."):
                        try:
                            # Set model path environment
                            os.environ["CUSTOM_MODEL_PATH"] = model_path
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Model YÃ¼kleme ZamanlamasÄ± (DiÄŸer Model TÃ¼rÃ¼) >>>
                            load_start_time_other = datetime.now()
                            # <<< GeliÅŸtirme Sonu: Model YÃ¼kleme ZamanlamasÄ± (DiÄŸer Model TÃ¼rÃ¼) >>>
                            st.session_state.chatbot = PDFChatBot(
                                use_local_llm=True,
                                model_choice="custom_drive_model" # Bu model_choice'un LocalLLMService tarafÄ±ndan nasÄ±l ele alÄ±ndÄ±ÄŸÄ±na baÄŸlÄ±
                            )
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Model YÃ¼kleme ZamanlamasÄ± (DiÄŸer Model TÃ¼rÃ¼) >>>
                            load_end_time_other = datetime.now()
                            st.session_state.model_load_duration_seconds = (load_end_time_other - load_start_time_other).total_seconds()
                            # <<< GeliÅŸtirme Sonu: Model YÃ¼kleme ZamanlamasÄ± (DiÄŸer Model TÃ¼rÃ¼) >>>
                            
                            llm_info = st.session_state.chatbot.llm_service.get_service_info()
                            st.session_state.model_initialized = True
                            
                            st.success(f"""
                            âœ… **ChatBot baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!**
                            
                            **ğŸ¤– Model Bilgileri:**
                            - Path: {model_path}
                            - Durum: {llm_info['status']}
                            
                            ğŸ’¬ ArtÄ±k PDF yÃ¼kleyip soru sorabilirsiniz!
                            """)
                            
                        except Exception as e:
                            st.error(f"âŒ ChatBot baÅŸlatÄ±lamadÄ±: {str(e)}")
        
        # PDF Upload section - ORTAK ALAN (her iki model tÃ¼rÃ¼ iÃ§in de)
        st.divider()  # Visual separator
        st.header("ğŸ“ Dosya YÃ¼kle")
        
        # File type selection
        file_type = st.selectbox(
            "ğŸ“„ Dosya tÃ¼rÃ¼ seÃ§in:",
            ["PDF", "Excel (XLS/XLSX)"],
            help="YÃ¼klemek istediÄŸiniz dosya tÃ¼rÃ¼nÃ¼ seÃ§in"
        )
        
        if file_type == "PDF":
            uploaded_file = st.file_uploader(
                "PDF dosyasÄ± seÃ§in:",
                type=['pdf'],
                help="Analiz etmek istediÄŸiniz PDF dosyasÄ±nÄ± yÃ¼kleyin"
            )
            
            if st.button("ğŸ“¤ PDF'i YÃ¼kle ve Ä°ÅŸle", type="primary"):
                if not st.session_state.model_initialized:
                    st.error("âŒ Ã–nce ChatBot'u baÅŸlatÄ±n")
                elif uploaded_file is not None:
                    with st.spinner("PDF iÅŸleniyor..."):
                        try:
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Dosya Ä°ÅŸleme SÃ¼resini Alma >>>
                            result_summary, duration_secs = st.session_state.chatbot.process_pdf_file(uploaded_file)
                            if duration_secs is not None:
                                st.session_state.last_file_processing_duration_seconds = duration_secs
                            # <<< GeliÅŸtirme Sonu: Dosya Ä°ÅŸleme SÃ¼resini Alma >>>
                            if "âœ…" in result_summary:
                                if uploaded_file.name not in st.session_state.processed_files:
                                    st.session_state.processed_files.append(uploaded_file.name)
                                st.success(result_summary)
                            else:
                                st.error(result_summary)
                        except Exception as e:
                            st.error(f"PDF iÅŸleme hatasÄ±: {e}")
                else:
                    st.warning("LÃ¼tfen bir PDF dosyasÄ± seÃ§in")
        
        else:  # Excel
            uploaded_file = st.file_uploader(
                "Excel dosyasÄ± seÃ§in:",
                type=['xls', 'xlsx', 'xlsm'],
                help="Analiz etmek istediÄŸiniz Excel dosyasÄ±nÄ± yÃ¼kleyin"
            )
            
            if st.button("ğŸ“Š Excel'i YÃ¼kle ve Ä°ÅŸle", type="primary"):
                if not st.session_state.model_initialized:
                    st.error("âŒ Ã–nce ChatBot'u baÅŸlatÄ±n")
                elif uploaded_file is not None:
                    with st.spinner("Excel dosyasÄ± iÅŸleniyor..."):
                        try:
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Dosya Ä°ÅŸleme SÃ¼resini Alma >>>
                            result_summary, duration_secs = st.session_state.chatbot.process_excel_file(uploaded_file)
                            if duration_secs is not None:
                                st.session_state.last_file_processing_duration_seconds = duration_secs
                            # <<< GeliÅŸtirme Sonu: Dosya Ä°ÅŸleme SÃ¼resini Alma >>>
                            if "âœ…" in result_summary:
                                if uploaded_file.name not in st.session_state.processed_files:
                                    st.session_state.processed_files.append(uploaded_file.name)
                                st.success(result_summary)
                            else:
                                st.error(result_summary)
                        except Exception as e:
                            st.error(f"Excel iÅŸleme hatasÄ±: {e}")
                else:
                    st.warning("LÃ¼tfen bir Excel dosyasÄ± seÃ§in")
        
        # Show processed files - ORTAK ALAN
        if st.session_state.processed_files:
            st.header("ğŸ“š Ä°ÅŸlenen Dosyalar")
            for file_name in st.session_state.processed_files:
                st.write(f"âœ… {file_name}")
        
        # Stats section - ORTAK ALAN
        st.header("ğŸ“Š Ä°statistikler")
        if st.session_state.chatbot:
            try:
                stats_text = st.session_state.chatbot.get_stats()
                st.markdown(stats_text)
            except:
                st.info("Ä°statistik alÄ±namadÄ±")
        else:
            st.info("ChatBot baÅŸlatÄ±lmadÄ±")
    
    # Main content area with simplified tabs
    tab1, tab2 = st.tabs(["ğŸ’¬ Sohbet", "â„¹ï¸ Rehber"])
    
    with tab1:
        # Chat interface
        st.header("ğŸ’¬ Sohbet")
        
        # Show information about loaded data
        if st.session_state.processed_files:
            st.info(f"ğŸ“š YÃ¼klenen dosyalar: {', '.join(st.session_state.processed_files)}")
        elif st.session_state.model_initialized:
            st.warning("âš ï¸ HenÃ¼z PDF yÃ¼klenmedi. Sol menÃ¼den PDF yÃ¼kleyin.")
        
        # Check vector store status for additional safety
        if st.session_state.chatbot:
            try:
                retrieval_stats = st.session_state.chatbot.retrieval_service.get_retrieval_stats()
                total_docs = retrieval_stats['vector_store_stats'].get('total_documents', 0)
                if total_docs == 0 and st.session_state.processed_files:
                    st.error("âš ï¸ Veriler kaybolmuÅŸ gibi gÃ¶rÃ¼nÃ¼yor. LÃ¼tfen PDF'leri tekrar yÃ¼kleyin.")
                elif total_docs > 0:
                    st.success(f"âœ… Vector store'da {total_docs} chunk hazÄ±r")
            except:
                pass
        
        # Display chat history
        for i, (user_msg, bot_msg) in enumerate(st.session_state.chat_history):
            with st.chat_message("user"):
                st.write(user_msg)
            with st.chat_message("assistant"):
                st.write(bot_msg)
        
        # Chat input with proper handling
        query = st.chat_input("Sorunuzu yazÄ±n...", key="chat_input")
        
        # Handle query submission
        if query:
            if not st.session_state.model_initialized:
                st.error("âŒ Ã–nce ChatBot'u baÅŸlatÄ±n (Sol menÃ¼den)")
            elif not st.session_state.processed_files:
                st.error("âŒ Ã–nce PDF dosyasÄ± yÃ¼kleyin (Sol menÃ¼den)")
            else:
                try:
                    retrieval_stats = st.session_state.chatbot.retrieval_service.get_retrieval_stats()
                    total_docs = retrieval_stats['vector_store_stats'].get('total_documents', 0)
                    if total_docs == 0:
                        st.error("âŒ Vector store'da veri bulunamadÄ±. PDF'leri tekrar yÃ¼kleyin.")
                        st.stop()
                except:
                    st.error("âŒ Vector store durumu kontrol edilemedi.")
                    st.stop()
                
                with st.chat_message("user"):
                    st.write(query)
                
                with st.chat_message("assistant"):
                    with st.spinner("Cevap Ã¼retiliyor..."):
                        try:
                            # <<< GeliÅŸtirme BaÅŸlangÄ±cÄ±: Sorgu ve LLM SÃ¼relerini Alma >>>
                            response_text, total_duration, llm_duration = st.session_state.chatbot.generate_single_response(query)
                            if total_duration is not None:
                                st.session_state.last_query_response_duration_seconds = total_duration
                            if llm_duration is not None:
                                st.session_state.last_llm_inference_duration_seconds = llm_duration
                            # <<< GeliÅŸtirme Sonu: Sorgu ve LLM SÃ¼relerini Alma >>>
                            st.write(response_text)
                            
                            st.session_state.chat_history.append([query, response_text])
                            
                        except Exception as e:
                            error_msg = f"âŒ Hata: {str(e)}"
                            st.error(error_msg)
                            st.session_state.chat_history.append([query, error_msg])
        
        # Clear chat button
        if st.button("ğŸ—‘ï¸ Sohbeti Temizle"):
            st.session_state.chat_history = []
            if st.session_state.chatbot:
                st.session_state.chatbot.chat_history = []
            st.rerun()
    
    with tab2:
        # Usage guide
        st.header("â„¹ï¸ KullanÄ±m Rehberi")
        
        st.markdown("""
        ## ğŸš€ NasÄ±l KullanÄ±lÄ±r?
        
        ### 1. Model SeÃ§imi ve BaÅŸlatma
        **GGUF Model (Ã–nerilen):**
        - âœ… "GGUF Model Kullan" kutusunu iÅŸaretle  
        - ğŸ“ Model path'ini gir: `/content/drive/MyDrive/Colab Notebooks/kredi_rag_sistemi/backup/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf`
        - ğŸš€ "GGUF ChatBot'u BaÅŸlat" butonuna tÄ±kla
        
        **Normal HuggingFace Model:**
        - âŒ "GGUF Model Kullan" kutusunu kaldÄ±r
        - ğŸ“ HuggingFace model path'ini gir 
        - ğŸš€ "ChatBot'u BaÅŸlat" butonuna tÄ±kla
        
        ### 2. PDF YÃ¼kleme  
        - ğŸ“„ "PDF YÃ¼kle" bÃ¶lÃ¼mÃ¼nden PDF dosyanÄ±zÄ± seÃ§in
        - ğŸ“¤ "YÃ¼kle ve Ä°ÅŸle" butonuna tÄ±klayÄ±n
        - â³ Ä°ÅŸlem tamamlanana kadar bekleyin
        - **ğŸ“Š Tablolar otomatik olarak iÅŸlenir!**
        
        ### 2. Dosya YÃ¼kleme
        **PDF DosyalarÄ±:**
        - ğŸ“„ Dosya tÃ¼rÃ¼ olarak "PDF" seÃ§in
        - ğŸ“ PDF dosyanÄ±zÄ± yÃ¼kleyin
        - ğŸ“¤ "PDF'i YÃ¼kle ve Ä°ÅŸle" butonuna tÄ±klayÄ±n
        - **ğŸ“Š Tablolar otomatik olarak iÅŸlenir!**
        
        **Excel DosyalarÄ± (YENÄ°!):**
        - ğŸ“Š Dosya tÃ¼rÃ¼ olarak "Excel (XLS/XLSX)" seÃ§in
        - ğŸ“ .xls, .xlsx veya .xlsm dosyanÄ±zÄ± yÃ¼kleyin
        - ğŸ“¤ "Excel'i YÃ¼kle ve Ä°ÅŸle" butonuna tÄ±klayÄ±n
        - **ğŸ”¢ TÃ¼m sayfalar ve veriler otomatik iÅŸlenir!**
        
        ### 3. Soru Sorma
        - ğŸ’¬ "Sohbet" sekmesinde sorunuzu chat input'a yazÄ±n
        - â Enter'a basÄ±n veya gÃ¶nder butonuna tÄ±klayÄ±n
        - **ğŸ” Hem metin hem de tablo verilerinde arama yapar**
        """)
        
        with st.expander("ğŸ¦™ GGUF vs Normal Model"):
            st.markdown("""
            **ğŸ¯ GGUF Model AvantajlarÄ±:**
            - âœ… Daha hÄ±zlÄ± baÅŸlatma
            - âœ… Daha az RAM kullanÄ±mÄ±  
            - âœ… GPU acceleration ile hÄ±zlÄ±
            - âœ… Quantized (sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ) format
            - âœ… Colab'da stabil Ã§alÄ±ÅŸÄ±r
            
            **ğŸ“š Normal HuggingFace Model:**
            - âš ï¸ Daha fazla RAM gerekir
            - âš ï¸ Ä°lk yÃ¼kleme uzun sÃ¼rer
            - âš ï¸ HuggingFace token gerekebilir
            - âœ… Daha fazla model seÃ§eneÄŸi
            """)
        
        with st.expander("ğŸ“Š Tablo Ä°ÅŸleme Ã–zellikleri"):
            st.markdown("""
            **âœ… Desteklenen Tablo Ä°ÅŸlemleri:**
            - ğŸ” Tablo iÃ§eriÄŸi otomatik Ã§Ä±karÄ±lÄ±r
            - ğŸ“ Tablolar aranabilir metin formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
            - ğŸ§  LLM tablolardan bilgi Ã§Ä±karabilir
            - ğŸ“Š SayÄ±sal verileri analiz edebilir
            
            **ğŸ“‹ Ã–rnek Sorular:**
            - "Tablodaki en yÃ¼ksek deÄŸer nedir?"
            - "Hangi kategoride kaÃ§ adet var?"
            - "2023 yÄ±lÄ± verilerini gÃ¶ster"
            - "Finansal tablodan gelir kalemlerini listele"
            - "Cari dÃ¶nem toplam dÃ¶nen varlÄ±klar ne kadar?"
            
            **âš¡ Otomatik Ä°ÅŸlenir:**
            - PDF yÃ¼klediÄŸinizde tablolar otomatik bulunur
            - BoÅŸ tablolar filtrelenir
            - Her tablo ayrÄ± chunk olarak iÅŸlenir
            - Tablo metadatasÄ± korunur
            """)
        
        with st.expander("ğŸ“Š Excel Ä°ÅŸleme Ã–zellikleri (YENÄ°!)"):
            st.markdown("""
            **âœ… Desteklenen Excel FormatlarÄ±:**
            - ğŸ“„ .xls (Excel 97-2003)
            - ğŸ“„ .xlsx (Excel 2007+)
            - ğŸ“„ .xlsm (Macro-enabled Excel)
            
            **ğŸ” Otomatik Ä°ÅŸlenen Veriler:**
            - ğŸ“Š TÃ¼m sayfalar (sheets) okunur
            - ğŸ”¢ SayÄ±sal ve metin verileri ayrÄ± iÅŸlenir
            - ğŸ“ˆ Otomatik istatistikler (toplam, ortalama, min, max)
            - ğŸ“‹ SÃ¼tun adlarÄ± ve metadata korunur
            - ğŸ§¹ BoÅŸ satÄ±r/sÃ¼tunlar temizlenir
            
            **ğŸ“‹ Excel Sorgu Ã–rnekleri:**
            - "Hangi sayfalarda hangi veriler var?"
            - "Sheet1'deki toplam satÄ±r sayÄ±sÄ±?"
            - "En yÃ¼ksek maaÅŸ ne kadar?"
            - "Gelir tablosunu Ã¶zetle"
            - "SatÄ±ÅŸ verilerini analiz et"
            - "TÃ¼m sayfalardaki sayÄ±sal Ã¶zeti ver"
            - "Hangi sÃ¼tunlarda hangi tÃ¼rde veriler var?"
            
            **âš¡ GeliÅŸmiÅŸ Ã–zellikler:**
            - Her sayfa ayrÄ± chunk olarak iÅŸlenir
            - SayÄ±sal sÃ¼tunlar iÃ§in otomatik istatistik
            - Metin formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ aranabilir veri
            - Vector store'da metadata ile arama
            """)

        with st.expander("âš ï¸ Troubleshooting"):
            st.markdown("""
            **âŒ "GGUF Model bulunamadÄ±" hatasÄ±:**
            - Drive mount edilmiÅŸ mi kontrol edin
            - Path'in doÄŸru olduÄŸunu kontrol edin
            - Dosya izinlerini kontrol edin
            
            **ğŸŒ "Model yavaÅŸ yÃ¼kleniyor":**
            - Ä°lk yÃ¼kleme 2-5 dakika sÃ¼rer (normal)
            - GPU memory'yi kontrol edin
            - Colab Pro Plus kullanÄ±n
            
            **ğŸ’¾ "Memory error":**
            - Runtime'Ä± restart edin
            - GPU'yu kontrol edin: Runtime > Change runtime type > GPU
            
            **ğŸ“Š "PDF yÃ¼klenmiyor":**
            - Ã–nce model baÅŸlatÄ±lmÄ±ÅŸ olmalÄ±
            - PDF dosyasÄ± bozuk olmasÄ±n
            - BÃ¼yÃ¼k dosyalar uzun sÃ¼rebilir
            
            **ğŸ” "Cevap alamÄ±yorum":**
            - Vector store'da veri var mÄ± kontrol edin
            - PDF'yi tekrar yÃ¼kleyin
            - FarklÄ± soru ifadesi deneyin
            """)

if __name__ == "__main__":
    # Check for required libraries
    try:
        import streamlit
        logger.info("âœ… Streamlit library found")
    except ImportError:
        logger.error("âŒ Streamlit not installed. Run: pip install streamlit")
        exit(1)
    
    # Run the main app
    main() 