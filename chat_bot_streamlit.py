#!/usr/bin/env python3
"""
PDF ChatBot - Streamlit Implementation
Interactive chat interface with PDF upload and question answering
Supports Local LLM models (Llama & Mistral) via HuggingFace
"""

import os
import logging
import streamlit as st
from typing import List, Optional, Tuple
from dataclasses import dataclass
import tempfile
import json
from datetime import datetime
import time

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="PDF ChatBot - Local LLM",
    page_icon="ğŸ“„ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 1rem;
        color: #1e3a8a;
    }
    .status-success {
        padding: 10px;
        border-radius: 5px;
        background-color: #d1fae5;
        border-left: 4px solid #10b981;
        margin: 10px 0;
    }
    .status-error {
        padding: 10px;
        border-radius: 5px;
        background-color: #fee2e2;
        border-left: 4px solid #ef4444;
        margin: 10px 0;
    }
    .status-warning {
        padding: 10px;
        border-radius: 5px;
        background-color: #fef3c7;
        border-left: 4px solid #f59e0b;
        margin: 10px 0;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border: 1px solid #e5e7eb;
    }
    .user-message {
        background-color: #dbeafe;
        margin-left: 2rem;
    }
    .assistant-message {
        background-color: #f3f4f6;
        margin-right: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# Import our custom modules
from pdf_processor import PDFProcessor
from text_processor import TextProcessor
from vector_store import VectorStore, RetrievalService

# Try to import local LLM service
try:
    from llm_service_local import LocalLLMService, create_optimized_llm_service, RECOMMENDED_MODELS
    LOCAL_LLM_AVAILABLE = True
    logger.info("âœ… Local LLM service available")
except ImportError:
    LOCAL_LLM_AVAILABLE = False
    logger.warning("âš ï¸ Local LLM service not available")

@dataclass
class ChatMessage:
    """Container for chat messages"""
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: datetime
    sources: Optional[List[str]] = None

class LLMService:
    """
    LLM service supporting both OpenAI API and Local models
    """
    
    def __init__(self, 
                 use_local: bool = True,
                 model_choice: str = "llama_3_1_8b",
                 api_key: Optional[str] = None):
        
        self.use_local = use_local and LOCAL_LLM_AVAILABLE
        self.model_choice = model_choice
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        self.local_service = None
        self.openai_client = None
        
        self._initialize_service()
    
    def _initialize_service(self):
        """Initialize the appropriate LLM service"""
        
        if self.use_local:
            try:
                logger.info(f"ğŸš€ Initializing local LLM: {self.model_choice}")
                self.local_service = create_optimized_llm_service(self.model_choice)
                logger.info("âœ… Local LLM service initialized")
                return
            except Exception as e:
                logger.error(f"âŒ Local LLM initialization failed: {e}")
                logger.info("ğŸ”„ Falling back to OpenAI API...")
        
        # Fallback to OpenAI
        if self.api_key:
            try:
                import openai
                self.openai_client = openai.OpenAI(api_key=self.api_key)
                logger.info("âœ… OpenAI client initialized")
            except ImportError:
                logger.warning("OpenAI library not installed")
            except Exception as e:
                logger.warning(f"OpenAI client initialization failed: {e}")
        else:
            logger.warning("No OpenAI API key provided")
    
    def generate_response(self, query: str, context: str, chat_history: List[ChatMessage] = None) -> str:
        """Generate response using available LLM service"""
        
        # Try local LLM first
        if self.local_service:
            try:
                return self.local_service.generate_response(query, context, chat_history)
            except Exception as e:
                logger.error(f"Local LLM failed: {e}")
        
        # Try OpenAI API
        if self.openai_client:
            try:
                return self._generate_openai_response(query, context, chat_history)
            except Exception as e:
                logger.error(f"OpenAI API failed: {e}")
        
        # Fallback response
        return self._generate_fallback_response(query, context)
    
    def _generate_openai_response(self, query: str, context: str, chat_history: List[ChatMessage] = None) -> str:
        """Generate response using OpenAI API"""
        
        system_prompt = """Sen finansal dokÃ¼manlarÄ± analiz eden uzman bir asistansÄ±n. 
        Sana verilen baÄŸlam bilgilerini kullanarak kullanÄ±cÄ±nÄ±n sorularÄ±nÄ± TÃ¼rkÃ§e olarak cevapla.
        
        Kurallar:
        - Sadece verilen baÄŸlam bilgilerini kullan
        - EÄŸer baÄŸlamda cevap yoksa "Bu bilgi dokÃ¼manlarda bulunmuyor" de
        - Finansal terimleri doÄŸru kullan
        - KaynaklarÄ± belirt
        - KÄ±sa ve net cevaplar ver
        """
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if chat_history:
            for msg in chat_history[-5:]:
                messages.append({"role": msg.role, "content": msg.content})
        
        user_message = f"""
        BaÄŸlam Bilgileri:
        {context}
        
        KullanÄ±cÄ± Sorusu:
        {query}
        """
        
        messages.append({"role": "user", "content": user_message})
        
        response = self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def _generate_fallback_response(self, query: str, context: str) -> str:
        """Fallback response when no LLM available"""
        
        if context:
            return f"""
            **[Demo Modu - LLM servisi mevcut deÄŸil]**
            
            Sorunuz: {query}
            
            Bulunan Ä°lgili Bilgiler:
            {context[:500]}...
            
            ğŸ’¡ LLM servisi aktif olduÄŸunda bu bilgileri analiz ederek detaylÄ± cevap veririm.
            """
        else:
            return "Bu konuda dokÃ¼manlarda ilgili bilgi bulunamadÄ±."
    
    def get_service_info(self) -> dict:
        """Get information about the current LLM service"""
        
        info = {
            "service_type": "unknown",
            "model_name": "none",
            "status": "not_available"
        }
        
        if self.local_service:
            local_info = self.local_service.get_model_info()
            info.update({
                "service_type": "local",
                "model_name": local_info["model_name"],
                "status": "active" if local_info["model_loaded"] else "failed",
                "device": local_info.get("device", "unknown"),
                "gpu_info": local_info.get("gpu_name", "N/A")
            })
        elif self.openai_client:
            info.update({
                "service_type": "openai_api",
                "model_name": "gpt-3.5-turbo",
                "status": "active"
            })
        
        return info

class PDFChatBot:
    """Main ChatBot class with Local LLM support"""
    
    def __init__(self, use_local_llm: bool = True, model_choice: str = "llama_3_1_8b"):
        # Initialize components
        self.pdf_processor = PDFProcessor()
        self.text_processor = TextProcessor()
        self.vector_store = VectorStore()
        self.retrieval_service = RetrievalService(self.vector_store)
        
        # Initialize LLM service
        self.llm_service = LLMService(
            use_local=use_local_llm,
            model_choice=model_choice
        )
        
        # Chat history
        self.chat_history: List[ChatMessage] = []
        
        # Stats
        self.stats = {
            "total_queries": 0,
            "successful_responses": 0,
            "documents_processed": 0,
            "chunks_in_database": 0
        }
        
        logger.info("âœ… PDF ChatBot initialized")
    
    def process_pdf_file(self, pdf_file) -> str:
        """Process uploaded PDF file"""
        try:
            if pdf_file is None:
                return "âŒ Dosya seÃ§ilmedi"
            
            # Save uploaded file temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(pdf_file.read())
                tmp_path = tmp_file.name
            
            try:
                # Process PDF
                logger.info(f"Processing PDF: {tmp_path}")
                
                # Extract text
                pdf_text = self.pdf_processor.extract_text(tmp_path)
                if not pdf_text.strip():
                    return "âŒ PDF'den metin Ã§Ä±karÄ±lamadÄ±"
                
                # Process text into chunks
                chunks = self.text_processor.create_chunks(pdf_text)
                logger.info(f"Created {len(chunks)} text chunks")
                
                # Store in vector database
                self.vector_store.add_documents(chunks)
                
                # Update stats
                self.stats["documents_processed"] += 1
                self.stats["chunks_in_database"] = len(chunks)
                
                logger.info("âœ… PDF processing completed")
                
                return f"""
                âœ… **PDF baÅŸarÄ±yla iÅŸlendi!**
                
                ğŸ“Š **Ä°ÅŸlem DetaylarÄ±:**
                - ğŸ“„ Metin uzunluÄŸu: {len(pdf_text)} karakter
                - ğŸ”¤ ParÃ§a sayÄ±sÄ±: {len(chunks)}
                - ğŸ’¾ VeritabanÄ±na eklendi
                
                ğŸ’¬ ArtÄ±k bu dokÃ¼man hakkÄ±nda sorular sorabilirsiniz!
                """
                
            finally:
                # Clean up temporary file
                os.unlink(tmp_path)
                
        except Exception as e:
            logger.error(f"PDF processing failed: {e}")
            return f"âŒ PDF iÅŸleme hatasÄ±: {str(e)}"
    
    def chat(self, query: str) -> str:
        """Process chat query and return response"""
        
        if not query.strip():
            return "âŒ LÃ¼tfen bir soru yazÄ±n"
        
        try:
            self.stats["total_queries"] += 1
            
            # Retrieve relevant context
            relevant_docs = self.retrieval_service.retrieve_relevant_chunks(query, top_k=3)
            context = "\n\n".join(relevant_docs) if relevant_docs else ""
            
            # Generate response
            response = self.llm_service.generate_response(query, context, self.chat_history)
            
            # Add to chat history
            self.chat_history.append(ChatMessage(
                role="user",
                content=query,
                timestamp=datetime.now()
            ))
            
            self.chat_history.append(ChatMessage(
                role="assistant", 
                content=response,
                timestamp=datetime.now(),
                sources=relevant_docs[:2] if relevant_docs else None
            ))
            
            self.stats["successful_responses"] += 1
            
            return response
            
        except Exception as e:
            logger.error(f"Chat processing failed: {e}")
            return f"âŒ Soru iÅŸleme hatasÄ±: {str(e)}"
    
    def get_stats(self) -> dict:
        """Get chatbot statistics"""
        return self.stats.copy()

# Initialize session state
def init_session_state():
    """Initialize Streamlit session state"""
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'model_initialized' not in st.session_state:
        st.session_state.model_initialized = False

def display_chat_message(message, is_user=True):
    """Display a chat message with proper styling"""
    css_class = "user-message" if is_user else "assistant-message"
    role = "ğŸ™‹â€â™‚ï¸ Siz" if is_user else "ğŸ¤– ChatBot"
    
    st.markdown(f"""
    <div class="chat-message {css_class}">
        <strong>{role}:</strong><br>
        {message}
    </div>
    """, unsafe_allow_html=True)

def main():
    """Main Streamlit application"""
    
    # Initialize session state
    init_session_state()
    
    # Header
    st.markdown('<div class="main-header">ğŸ“„ğŸ’¬ PDF ChatBot - Local LLM Destekli</div>', unsafe_allow_html=True)
    
    st.markdown("""
    **Finansal dokÃ¼manlarÄ±nÄ±zÄ± yÃ¼kleyin ve Local LLM ile sorularÄ±nÄ±zÄ± sorun!**
    
    - ğŸ¤– Llama 3.1 8B veya Mistral 7B modelleri
    - ğŸ“„ PDF dosyalarÄ±nÄ±zÄ± yÃ¼kleyin  
    - ğŸ§  AkÄ±llÄ± finansal analiz
    - ğŸ”’ Tamamen local (internet gerekmez)
    """)
    
    # Sidebar for model configuration
    with st.sidebar:
        st.header("âš™ï¸ Model AyarlarÄ±")
        
        # Model selection
        if LOCAL_LLM_AVAILABLE:
            model_choices = list(RECOMMENDED_MODELS.keys())
            default_model = "llama_3_1_8b"
        else:
            model_choices = ["local_not_available"]
            default_model = "local_not_available"
        
        selected_model = st.selectbox(
            "ğŸ¯ LLM Model SeÃ§in:",
            model_choices,
            index=model_choices.index(default_model) if default_model in model_choices else 0
        )
        
        # Display model info
        if LOCAL_LLM_AVAILABLE and selected_model in RECOMMENDED_MODELS:
            config = RECOMMENDED_MODELS[selected_model]
            st.markdown(f"""
            **ğŸ“Š Model Bilgileri:**
            - **Ä°sim:** {config['name']}
            - **Memory:** {config['memory']}
            - **AÃ§Ä±klama:** {config['description']}
            """)
        elif not LOCAL_LLM_AVAILABLE:
            st.error("âŒ **Local LLM mevcut deÄŸil**\n\nGerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:\n`pip install transformers accelerate torch`")
        
        # Initialize button
        if st.button("ğŸš€ ChatBot'u BaÅŸlat", type="primary", disabled=st.session_state.model_initialized):
            if not LOCAL_LLM_AVAILABLE and selected_model != "local_not_available":
                st.error("Local LLM servisi mevcut deÄŸil!")
            else:
                with st.spinner("Model yÃ¼kleniyor..."):
                    try:
                        st.session_state.chatbot = PDFChatBot(
                            use_local_llm=LOCAL_LLM_AVAILABLE,
                            model_choice=selected_model if LOCAL_LLM_AVAILABLE else "fallback"
                        )
                        
                        llm_info = st.session_state.chatbot.llm_service.get_service_info()
                        st.session_state.model_initialized = True
                        
                        st.success(f"""
                        âœ… **ChatBot baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!**
                        
                        **ğŸ¤– Aktif LLM:**
                        - Tip: {llm_info['service_type']}
                        - Model: {llm_info['model_name']}
                        - Durum: {llm_info['status']}
                        
                        ğŸ’¬ ArtÄ±k PDF yÃ¼kleyip soru sorabilirsiniz!
                        """)
                        
                    except Exception as e:
                        st.error(f"âŒ ChatBot baÅŸlatÄ±lamadÄ±: {str(e)}")
        
        # Stats section
        st.header("ğŸ“Š Ä°statistikler")
        if st.session_state.chatbot:
            stats = st.session_state.chatbot.get_stats()
            st.metric("Total Queries", stats["total_queries"])
            st.metric("Successful Responses", stats["successful_responses"])
            st.metric("Documents Processed", stats["documents_processed"])
            st.metric("Chunks in Database", stats["chunks_in_database"])
        else:
            st.info("ChatBot baÅŸlatÄ±lmadÄ±")
    
    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Chat interface
        st.header("ğŸ’¬ Sohbet")
        
        # Display chat history
        chat_container = st.container()
        with chat_container:
            for i, message in enumerate(st.session_state.chat_history):
                if i % 2 == 0:  # User message
                    display_chat_message(message, is_user=True)
                else:  # Assistant message
                    display_chat_message(message, is_user=False)
        
        # Chat input
        user_input = st.text_input(
            "Sorunuzu yazÄ±n:",
            placeholder="Ã–rn: Åirketin 2024 yÄ±lÄ± net kÃ¢rÄ± ne kadar?",
            key="user_input"
        )
        
        col_send, col_clear = st.columns([1, 1])
        
        with col_send:
            if st.button("ğŸ“¤ GÃ¶nder", type="primary"):
                if not st.session_state.model_initialized:
                    st.error("âŒ Ã–nce ChatBot'u baÅŸlatÄ±n (Sol menÃ¼den)")
                elif user_input.strip():
                    with st.spinner("Cevap Ã¼retiliyor..."):
                        response = st.session_state.chatbot.chat(user_input)
                        st.session_state.chat_history.extend([user_input, response])
                        st.rerun()
        
        with col_clear:
            if st.button("ğŸ—‘ï¸ Sohbeti Temizle"):
                st.session_state.chat_history = []
                if st.session_state.chatbot:
                    st.session_state.chatbot.chat_history = []
                st.rerun()
    
    with col2:
        # File upload section
        st.header("ğŸ“ PDF DosyasÄ± YÃ¼kle")
        
        uploaded_file = st.file_uploader(
            "PDF dosyasÄ± seÃ§in:",
            type=['pdf'],
            help="Analiz etmek istediÄŸiniz PDF dosyasÄ±nÄ± yÃ¼kleyin"
        )
        
        if st.button("ğŸ“¤ YÃ¼kle ve Ä°ÅŸle", type="primary"):
            if not st.session_state.model_initialized:
                st.error("âŒ Ã–nce ChatBot'u baÅŸlatÄ±n")
            elif uploaded_file is not None:
                with st.spinner("PDF iÅŸleniyor..."):
                    result = st.session_state.chatbot.process_pdf_file(uploaded_file)
                    if "âœ…" in result:
                        st.success(result)
                    else:
                        st.error(result)
            else:
                st.warning("LÃ¼tfen bir PDF dosyasÄ± seÃ§in")
        
        # Usage guide
        st.header("â„¹ï¸ KullanÄ±m Rehberi")
        
        with st.expander("ğŸš€ NasÄ±l KullanÄ±lÄ±r?"):
            st.markdown("""
            ### 1. Model SeÃ§imi
            - Sol menÃ¼den istediÄŸiniz modeli seÃ§in
            - **Ã–nerilen:** Llama 3.1 8B (dengeli performans)
            - **HÄ±zlÄ±:** Mistral 7B (dÃ¼ÅŸÃ¼k memory)
            - "ğŸš€ ChatBot'u BaÅŸlat" butonuna tÄ±klayÄ±n
            
            ### 2. PDF YÃ¼kleme  
            - PDF dosyanÄ±zÄ± seÃ§in ve "ğŸ“¤ YÃ¼kle ve Ä°ÅŸle" butonuna tÄ±klayÄ±n
            - Ä°ÅŸlem tamamlanana kadar bekleyin
            
            ### 3. Soru Sorma
            - Sorunuzu metin kutusuna yazÄ±n
            - "ğŸ“¤ GÃ¶nder" butonuna tÄ±klayÄ±n
            """)
        
        with st.expander("ğŸ¯ Model Ã–nerileri"):
            st.markdown(f"""
            **ğŸ† Llama 3.1 8B** (Ã–nerilen)
            - Memory: ~16GB
            - TÃ¼rkÃ§e desteÄŸi mÃ¼kemmel
            - Finansal analiz iÃ§in optimize
            
            **âš¡ Mistral 7B** (HÄ±zlÄ±)
            - Memory: ~14GB  
            - Daha hÄ±zlÄ± inference
            - Kod Ã¼retme konusunda gÃ¼Ã§lÃ¼
            
            **ğŸ’ª Llama 3.1 70B** (GÃ¼Ã§lÃ¼)
            - Memory: ~40GB
            - En iyi reasoning
            - Sadece yÃ¼ksek GPU memory'de Ã§alÄ±ÅŸÄ±r
            
            **Local LLM Mevcut:** {"âœ… Evet" if LOCAL_LLM_AVAILABLE else "âŒ HayÄ±r"}
            """)

if __name__ == "__main__":
    main() 