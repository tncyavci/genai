#!/usr/bin/env python3
"""
A100 Optimization - Mevcut workflow'a eklemek iÃ§in
"""

import torch
import os

def apply_a100_optimizations():
    """A100 GPU optimizasyonlarÄ±nÄ± uygula"""
    print("ğŸ”¥ A100 GPU OptimizasyonlarÄ±")
    print("=" * 40)
    
    if not torch.cuda.is_available():
        print("âŒ GPU bulunamadÄ±!")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    print(f"âœ… GPU: {gpu_name}")
    print(f"ğŸ’¾ Memory: {gpu_memory:.1f}GB")
    
    # A100 iÃ§in Ã¶zel optimizasyonlar
    if "A100" in gpu_name:
        print("ğŸ¯ A100 DETECTED - Maximum optimizations!")
        
        # PyTorch optimizasyonlarÄ±
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.cuda.set_per_process_memory_fraction(0.9)
        
        # Environment variables
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
        os.environ["CUDA_CACHE_DISABLE"] = "0"
        
        # HuggingFace cache
        os.environ["HF_HOME"] = "/content/hf_cache"
        os.environ["TRANSFORMERS_CACHE"] = "/content/hf_cache"
        
        print("âš¡ cuDNN benchmark: ON")
        print("âš¡ TF32 matmul: ON") 
        print("âš¡ GPU memory: 90%")
        print("âš¡ CUDA optimizations: ON")
        print("âœ… A100 optimizations applied!")
        
        return True
    else:
        print(f"âš ï¸ GPU: {gpu_name} (Not A100)")
        print("ğŸ“ Basic optimizations applied")
        
        # Basic optimizasyonlar
        torch.backends.cudnn.benchmark = True
        torch.cuda.set_per_process_memory_fraction(0.8)
        
        return True

def check_model_path():
    """Model path kontrolÃ¼"""
    # Kendi model path'inizi buraya yazÄ±n
    model_paths = [
        "/content/drive/MyDrive/Colab Notebooks/kredi_rag_sistemi/backup/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "/content/drive/MyDrive/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "/content/drive/MyDrive/mistral-7b.gguf"
    ]
    
    print("\nğŸ“ Model path kontrolÃ¼:")
    
    for model_path in model_paths:
        if os.path.exists(model_path):
            size_gb = os.path.getsize(model_path) / (1024**3)
            print(f"âœ… Found: {os.path.basename(model_path)} ({size_gb:.1f}GB)")
            print(f"ğŸ“ Path: {model_path}")
            return model_path
    
    print("âŒ HiÃ§bir model bulunamadÄ±!")
    print("ğŸ“ Model path'lerini gÃ¼ncelle:")
    for path in model_paths:
        print(f"   - {path}")
    
    return None

if __name__ == "__main__":
    # A100 optimizasyonlarÄ±nÄ± uygula
    success = apply_a100_optimizations()
    
    if success:
        model_path = check_model_path()
        
        print("\nğŸš€ Optimizasyonlar tamamlandÄ±!")
        print("ğŸ“‹ SÄ±radaki adÄ±mlar:")
        print("1. Streamlit'i baÅŸlat (mevcut workflow)")
        print("2. ngrok tunnel oluÅŸtur (mevcut workflow)")
        
        if model_path:
            print(f"\nğŸ’¡ ChatBot UI'da kullan:")
            print(f"âœ… GGUF Model Kullan: âœ“")
            print(f"ğŸ“ Model path: {model_path}")
        else:
            print("\nâš ï¸ Model path'ini manuel gir!")
    else:
        print("âŒ Optimizasyonlar baÅŸarÄ±sÄ±z!") 