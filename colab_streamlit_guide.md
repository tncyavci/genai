# ğŸš€ PDF ChatBot - Google Colab'da Streamlit ile Ã‡alÄ±ÅŸtÄ±rma

## âš¡ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (2 dakikada Ã§alÄ±ÅŸÄ±r!)

### 1. Colab Notebook OluÅŸtur
```python
# Google Colab'da yeni notebook oluÅŸturun
# Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU (T4/V100/A100)
```

### 2. Projeyi Klonla ve Kurulum Yap
```python
# Projeyi indirin
!git clone YOUR_REPO_URL
%cd pdf-chatbot

# Ã–zel Colab setup'Ä± Ã§alÄ±ÅŸtÄ±rÄ±n (dependency conflicts'i Ã§Ã¶zer)
!python colab_setup.py

# Runtime'Ä± restart edin (Ã¶nemli!)
# Runtime â†’ Restart runtime
```

### 3. Streamlit ChatBot'u BaÅŸlatÄ±n
```python
# Streamlit uygulamasÄ±nÄ± baÅŸlatÄ±n
!streamlit run chat_bot.py --server.port 8501 &

# Alternatif: Debug mode ile
!streamlit run chat_bot.py --server.port 8501 --logger.level debug &
```

### 4. Public URL AlÄ±n
```python
# Streamlit otomatik olarak public URL oluÅŸturur
# Terminal output'unda ÅŸÃ¶yle bir link gÃ¶receksiniz:
# External URL: https://xyz-abc123.streamlit.app

# Alternatif: ngrok ile custom domain
!pip install pyngrok
from pyngrok import ngrok
public_url = ngrok.connect(8501)
print(f"ğŸŒ Public URL: {public_url}")
```

## ğŸ”§ Model Kurulumu

### HuggingFace Token (Llama modeller iÃ§in)
```python
# 1. Token alÄ±n: https://huggingface.co/settings/tokens
# 2. Login yapÄ±n:
from huggingface_hub import login
login(token="hf_your_token_here")
```

### GPU Memory KontrolÃ¼
```python
import torch
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"ğŸš€ GPU: {gpu_name}")
    print(f"ğŸ’¾ Memory: {gpu_memory:.1f}GB")
    
    # Model Ã¶nerileri
    if gpu_memory > 20:
        print("ğŸ¯ Recommended: Llama 3.1 8B")
    elif gpu_memory > 15:
        print("ğŸ¯ Recommended: Mistral 7B")
    else:
        print("ğŸ¯ Recommended: Llama 3.2 3B (quantized)")
else:
    print("âš ï¸ GPU bulunamadÄ± - CPU Ã§ok yavaÅŸ olacak")
```

## ğŸ“± KullanÄ±m Rehberi

### 1. Model SeÃ§imi
- Streamlit arayÃ¼zÃ¼nde sidebar'dan model seÃ§in
- GPU memory'nize gÃ¶re model seÃ§in
- "ğŸš€ ChatBot'u BaÅŸlat" butonuna tÄ±klayÄ±n

### 2. PDF YÃ¼kleme
- "PDF DosyasÄ± YÃ¼kleyin" bÃ¶lÃ¼mÃ¼nden dosya seÃ§in
- Dosya iÅŸlenmeyi bekleyin (1-2 dakika)
- âœ… iÅŸareti gÃ¶rÃ¼nce hazÄ±r

### 3. Sohbet Etme
- "Sorunuzu yazÄ±n..." kutusuna sorunuzu yazÄ±n
- Enter'a basÄ±n veya Send butonuna tÄ±klayÄ±n
- Model yanÄ±tÄ±nÄ± bekleyin

## âš ï¸ YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mler

### "Streamlit bulunamadÄ±" hatasÄ±
```python
# Ã‡Ã¶zÃ¼m:
!pip install streamlit>=1.28.0
# Runtime â†’ Restart runtime
```

### "CUDA out of memory" hatasÄ±
```python
# Ã‡Ã¶zÃ¼m 1: Memory temizle
import gc
import torch
gc.collect()
torch.cuda.empty_cache()

# Ã‡Ã¶zÃ¼m 2: Daha kÃ¼Ã§Ã¼k model seÃ§
# Llama 3.2 3B veya quantized model kullan
```

### Streamlit baÄŸlanamÄ±yor
```python
# Port kontrolÃ¼
!netstat -tlnp | grep :8501

# Streamlit process'i Ã¶ldÃ¼r ve yeniden baÅŸlat
!pkill -f streamlit
!streamlit run chat_bot.py --server.port 8501 &
```

### Model yÃ¼klenmiyor
```python
# Internet baÄŸlantÄ±sÄ±nÄ± kontrol edin
!ping huggingface.co

# Cache temizle
!rm -rf ~/.cache/huggingface/
!rm -rf /content/hf_cache/

# Token'Ä± yeniden girin
from huggingface_hub import login
login(token="your_token")
```

## ğŸ’¡ Pro Tips

### 1. Model Cache'i Kaydet
```python
# Google Drive'a mount et
from google.colab import drive
drive.mount('/content/drive')

# Cache directory'yi drive'a yÃ¶nlendir
import os
os.environ["HF_HOME"] = "/content/drive/MyDrive/hf_cache"
os.environ["TRANSFORMERS_CACHE"] = "/content/drive/MyDrive/hf_cache"
```

### 2. Background'da Ã‡alÄ±ÅŸtÄ±r
```python
# Nohup ile background
!nohup streamlit run chat_bot.py --server.port 8501 > streamlit.log 2>&1 &

# Log'larÄ± takip et
!tail -f streamlit.log
```

### 3. Memory Monitoring
```python
# GPU kullanÄ±mÄ±nÄ± izle
!watch -n 2 nvidia-smi

# Streamlit memory kullanÄ±mÄ±
import psutil
import os
process = psutil.Process(os.getpid())
print(f"Memory usage: {process.memory_info().rss / 1024 / 1024:.1f} MB")
```

### 4. Session State Koruma
```python
# Streamlit session state'i korumak iÃ§in
# UygulamayÄ± yeniden baÅŸlatmadan model deÄŸiÅŸtirme
# Settings â†’ Clear Cache â†’ Restart
```

## ğŸ“Š Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | Memory | Speed | Quality | Colab Uyumlu |
|-------|--------|-------|---------|---------------|
| Llama 3.1 8B | 16GB | Orta | YÃ¼ksek | âœ… V100/A100 |
| Mistral 7B | 14GB | HÄ±zlÄ± | Ä°yi | âœ… T4/V100 |
| Llama 3.2 3B | 6GB | HÄ±zlÄ± | Orta | âœ… T4 |
| Quantized 4-bit | 4GB | Orta | Orta | âœ… T4 |

## ğŸ”„ HÄ±zlÄ± Komutlar

```python
# Streamlit'i yeniden baÅŸlat
def restart_streamlit():
    !pkill -f streamlit
    !streamlit run chat_bot.py --server.port 8501 &

# Memory temizle
def clear_gpu_memory():
    import gc
    import torch
    gc.collect()
    torch.cuda.empty_cache()
    print("ğŸ§¹ GPU memory cleared!")

# Process kontrolÃ¼
def check_streamlit():
    !ps aux | grep streamlit
    !netstat -tlnp | grep :8501

# URL'i gÃ¶ster
def show_url():
    from pyngrok import ngrok
    tunnels = ngrok.get_tunnels()
    for tunnel in tunnels:
        print(f"ğŸŒ URL: {tunnel.public_url}")
```

## ğŸ¯ Streamlit vs Gradio AvantajlarÄ±

**Streamlit AvantajlarÄ±:**
- âœ… Dependency conflict'leri yok
- âœ… Daha stabil session management
- âœ… Better file upload handling
- âœ… Professional UI/UX
- âœ… Real-time updates
- âœ… Better mobile support

**Gradio SorunlarÄ± (neden deÄŸiÅŸtirdik):**
- âŒ Dependency conflict'leri
- âŒ Session timeout sorunlarÄ±
- âŒ File upload bugs
- âŒ Memory leak'ler

## ğŸš€ BaÅŸarÄ±lÄ± Ã‡alÄ±ÅŸma Kontrol Listesi

- [ ] âœ… GPU aktif (nvidia-smi)
- [ ] âœ… Dependencies kurulu (requirements)
- [ ] âœ… HuggingFace token set
- [ ] âœ… Streamlit Ã§alÄ±ÅŸÄ±yor (:8501)
- [ ] âœ… Public URL eriÅŸilebilir
- [ ] âœ… Model yÃ¼klendi
- [ ] âœ… PDF yÃ¼kleme Ã§alÄ±ÅŸÄ±yor
- [ ] âœ… Chat respond ediyor

---

**ğŸ‰ BaÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda arayÃ¼z ÅŸÃ¶yle gÃ¶rÃ¼necek:**
- Sidebar'da model seÃ§enekleri
- PDF upload area
- Chat interface
- Real-time response
- Source citations

**Colab'da Streamlit = MÃ¼kemmel kombi! ğŸš€** 